"""Callback Handler streams callback on new llm token in last agent response."""
import warnings
from queue import Queue
from typing import Any, Callable, Iterator, List, Optional, Type, Union, Dict
from pydantic import BaseModel, root_validator, validator

from langchain.agents.agent_types import AgentType
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import AgentFinish, OutputParserException

try:
    import tiktoken
except ImportError:
    raise ImportError(
        "Could not import tiktoken python package. "
        "This is needed in order to calculate detection_windows_size for StreamingLastResponseCallbackHandler"
        "Please install it with `pip install tiktoken`."
    )

class StreamingLastResponseCallbackHandler(BaseModel, BaseCallbackHandler):
    answer_prefix_phrases: List[str] = ["Final Answer:"]
    error_stop_streaming_phrases: List[str] = []
    case_sensitive_matching: bool = True
    output_stream_prefix: bool = False

    tiktoken_encoding: str = "cl100k_base"
    _enc: tiktoken.Encoding  #: :meta private:

    detection_queue_size: int = 1  # do not use Queue(maxsize=...), because it will block the queue.
    detection_queue: Queue[str] = Queue()
    output_queue: Queue[Union[str, Type[StopIteration], OutputParserException]] = Queue()

    is_streaming_answer: bool = False  # If the answer is reached, the streaming will be started.
    step_counter: int = 0
    callback_func: Callable[[Union[str, Type[StopIteration]]], None] = lambda new_token: None

    postprocess_sliding_window_step: int = 1
    postprocess_func: Optional[Callable[[List[str]], List[str]]] = None

    class Config:
        validate_assignment = True

    @validator("answer_prefix_phrases")
    def validate_answer_prefix_phrases(cls, v: List[str], values: Dict[str, Any]) -> List[str]:
        if not v:
            raise ValueError("answer_prefix_phrases cannot be empty.")
        sorted_phrases = sorted(v, key=len, reverse=True)
        if not values["case_sensitive_matching"]:
            sorted_phrases = [phrase.lower() for phrase in sorted_phrases]
        return sorted_phrases

    @validator("error_stop_streaming_phrases")
    def validate_error_stop_streaming_phrases(cls, v: List[str], values: Dict[str, Any]) -> List[str]:
        sorted_phrases = sorted(v, key=len, reverse=True)
        if not values["case_sensitive_matching"]:
            sorted_phrases = [phrase.lower() for phrase in sorted_phrases]
        return sorted_phrases

    @root_validator()
    def validate_detection_queue_size(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        _enc = tiktoken.get_encoding(values["tiktoken_encoding"])
        values["_enc"] = _enc

        max_answer_prefix_phrases_token_len = max(len(_enc.encode(prefix)) for prefix in values["answer_prefix_phrases"])
        max_error_stop_streaming_phrases_token_len = max(len(_enc.encode(phrase)) for phrase in values["error_stop_streaming_phrases"])
        values['detection_queue_size'] = max(values['detection_queue_size'], max_answer_prefix_phrases_token_len, max_error_stop_streaming_phrases_token_len, 1)

        return values

    def __iter__(self) -> Iterator[str]:  # FIXME: this overrides the parent class's Pydantic __iter__ method
        """
        This function is used when the callback handler is used as an iterator.
        """
        while True:
            # Pop out the output queue. If the output queue is empty, it will wait until the output queue is not empty.
            token = self.output_queue.get()

            if token is StopIteration:
                break
            elif isinstance(token, Exception):
                raise token
            elif isinstance(token, str):
                yield token
            else:
                raise TypeError(f"Unknown type: {type(token)} | {token}")

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:
        """
        This function is called when the agent finishes. It will flush the detection queue when there are no more tokens from on_llm_new_token.
        """
        super().on_agent_finish(finish, **kwargs)
        self._flush_detection_queue()

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """
        This function is called when a new token is generated by the LLM.
        """
        self.step_counter += 1
        self.detection_queue.put(token)

        if self.is_streaming_answer:
            # if the answer is reached, the streaming will be started.
            last_token = None
            if self.detection_queue.qsize() > self.detection_queue_size:
                if self.step_counter % self.postprocess_sliding_window_step == 0:
                    self._post_process_detection_queue()
                    self._check_abnormal_in_detection_queue()
                last_token = self.detection_queue.get()
                self._callback(last_token)

        elif self.detection_queue.qsize() > self.detection_queue_size:
            # if the answer is not reached, the detection queue will be checked.
            _answer_prefix_phrase = self._check_if_answer_reached()
            if _answer_prefix_phrase is not None:
                # remove all answer prefix tokens from the detection queue
                for _ in range(len(self._enc.encode(_answer_prefix_phrase))):
                    _token = self.detection_queue.get()
                    if self.output_stream_prefix:
                        # output the answer prefix token
                        self._callback(_token)
            else:
                # if the answer is not reached, the detection queue will pop out the oldest token.
                self.detection_queue.get()

    def postprocess(
        self,
        sliding_window_step: int = 1,
        window_size: Optional[int] = None,
    ) -> Callable[[Callable[[List[str]], List[str]]], Callable[[List[str]], List[str]]]:
        """
        Decorator to use as postprocess function.

        Args:
            sliding_window_step: Default is 1. This means that the postprocess_func will be applied to the detection queue after every new token.
            window_size: The window size to use for the postprocess_func. The actual used window size will be the maximum of window_size, max length of answer_prefix_phrases, and error_stop_streaming_phrases.
        """

        def _decorator(
            postprocess_func: Callable[[List[str]], List[str]]
        ) -> Callable[[List[str]], List[str]]:
            self.postprocess_func = postprocess_func
            self.postprocess_sliding_window_step = sliding_window_step
            self.detection_queue_size = max(
                self.detection_queue_size,
                window_size or 1,
            )
            return postprocess_func

        return _decorator

    def on_last_response_new_token(
        self,
    ) -> Callable[
        [Callable[[Union[str, Type[StopIteration]]], None]],
        Callable[[Union[str, Type[StopIteration]]], None],
    ]:
        """
        Decorator to use as callback function.
        """

        def _decorator(
            callback_func: Callable[[Union[str, Type[StopIteration]]], None]
        ) -> Callable[[Union[str, Type[StopIteration]]], None]:
            self.callback_func = callback_func
            return callback_func

        return _decorator

    @classmethod
    def from_agent_type(
        cls,
        agent: AgentType = AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        **kwargs: Any,
    ) -> "StreamingLastResponseCallbackHandler":
        """Create a callback handler for streaming in agents."""
        if agent == AgentType.ZERO_SHOT_REACT_DESCRIPTION:
            return cls(
                answer_prefix_phrases=[
                    "Final Answer:",
                ],
                **kwargs,
            )
        elif agent == AgentType.CONVERSATIONAL_REACT_DESCRIPTION:
            return cls(
                answer_prefix_phrases=[
                    "Do I need to use a tool? No\nAI:",
                    "Do I need to use a tool? No",
                ],
                error_stop_streaming_phrases=[
                    "Do I need to use a tool? No\nAction:",
                ],
                **kwargs,
            )

        elif agent == AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION:
            # TODO: Post processing remove last '"\n}' after final answer
            raise NotImplementedError
            return cls(
                answer_prefix_phrases=[
                    'Final Answer",\n    "action_input": "',
                    'Final Answer",\n  "action_input": "',
                ],
                **kwargs,
            )
        elif agent == AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION:
            # TODO: Post processing remove last '"\n}\n```' after final answer
            raise NotImplementedError
            return cls(
                answer_prefix_phrases=[
                    'Final Answer",\n    "action_input": "',
                    'Final Answer",\n  "action_input": "',
                ],
                **kwargs,
            )
        else:
            raise NotImplementedError

    def _post_process_detection_queue(self) -> None:
        """
        Post process on-the-fly the detection queue by using used-defined postprocess_func.
        This function will be called every postprocess_sliding_window_step.
        """
        if self.postprocess_func is not None:
            tokens = list(self.detection_queue.queue)
            tokens = self.postprocess_func(tokens)
            self.detection_queue.queue.clear()
            for token in tokens:
                self.detection_queue.put(token)

    def _check_abnormal_in_detection_queue(self) -> None:
        """
        Check if the detection queue is abnormal. If the detection queue is abnormal, it will raise OutputParserException and stop the streaming.
        Check by using error_stop_streaming_phrases. If the error_stop_streaming_phrases is detected, the streaming will be stopped.
        """
        sentence = "".join(self.detection_queue.queue)

        for error_stop_streaming_phrases in self.error_stop_streaming_phrases:
            if error_stop_streaming_phrases in sentence:
                self._callback(
                    OutputParserException(
                        f"Abnormal in detection queue detected. Detection queue: '{self.detection_queue.queue}'. Abnormal: '{error_stop_streaming_phrases}'"
                    )
                )

    def _flush_detection_queue(self) -> None:
        """
        Flush detection queue. This will be called when the agent is finished to flush all the remaining tokens in the detection queue.
        """
        while not self.detection_queue.empty():
            if not self.is_streaming_answer:
                _answer_prefix_phrase = self._check_if_answer_reached()
                if _answer_prefix_phrase is not None:
                    # remove all answer prefix tokens from detection queue
                    if not self.output_stream_prefix:
                        for _ in range(len(self._enc.encode(_answer_prefix_phrase))):
                            while self.detection_queue.queue[0] == "":
                                self.detection_queue.get()
                            self.detection_queue.get()
                    else:
                        for _ in range(len(self._enc.encode(_answer_prefix_phrase))):
                            self._callback(self.detection_queue.get())
                else:
                    self.detection_queue.get()
            else:
                self._callback(self.detection_queue.get())

        if not self.is_streaming_answer:
            warnings.warn(
                "StreamingLastResponseCallbackHandler is not streaming answer, but agent_finish is called."
            )

        self._callback(StopIteration)

    def _callback(
        self, text: Union[str, OutputParserException, Type[StopIteration]]
    ) -> None:
        """
        Callback function. It will put the text to the output queue, and call the callback_func.
        """
        if isinstance(text, OutputParserException):
            self.output_queue.put(text)
            raise text
        elif text is StopIteration:
            self.output_queue.put(text)
            self.callback_func(text)
        elif isinstance(text, str):
            self.output_queue.put(text)
            self.callback_func(text)
        else:
            raise TypeError(f"Unknown type: {type(text)} | {text}")

    def _check_if_answer_reached(self) -> Optional[str]:
        """
        Check if the answer is reached. If the answer is reached, it will return the answer prefix phrase.
        If the answer is not reached, it will return None.
        """
        if self.detection_queue.queue[0] == "":
            return None
        for _answer_prefix_str in self.answer_prefix_phrases:
            current_output = "".join(self.detection_queue.queue).strip()

            if self.case_sensitive_matching:
                current_output = current_output.lower()

            if current_output.startswith(_answer_prefix_str):
                self.is_streaming_answer = True
                return _answer_prefix_str
        return None
